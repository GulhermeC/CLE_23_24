#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>
#include <math.h>
#include <unistd.h>

int main (int argc, char *argv[])
{
   MPI_Comm presentComm, nextComm;
   MPI_Group presentGroup, nextGroup;
   int gMemb[8];
   int rank, nProc, nProcNow, length, nIter;
   int *sendData = NULL, *recData;
   int i, j;

  // Step 1: Initialize MPI
   MPI_Init (&argc, &argv);
   MPI_Comm_rank (MPI_COMM_WORLD, &rank);
   MPI_Comm_size (MPI_COMM_WORLD, &nProc);

  // Step 2: Validate the number of processes
   if (nProc > 8)
      { if (rank == 0) printf ("Too many processes! It should be a power of 2, less or equal to 8.\n");
        MPI_Finalize ();
        return EXIT_FAILURE;
      }

  // Step 3: Allocate memory and prepare data
   length = 1 << 24; // Calculating the length of the data
   nIter = (int) (log2 (nProc) + 1.1);
   recData = malloc (length * sizeof (int));
   if (rank == 0)
      { sendData = malloc (length * sizeof (int));
        printf ("Data to be scattered is being generated by process %d.\n", rank);
        for (i = 0; i < length; i++)
          sendData[i] = i;
      }

  // Step 4: Communication group and communicator setup
   nProcNow = nProc;
   presentComm = MPI_COMM_WORLD;
   MPI_Comm_group (presentComm, &presentGroup);

  // Step 5: Iterative communication process
   for (i = 0; i < 8; i++)
     gMemb[i] = i;
   for (j = 0; j < nIter; j++)
   { if (j > 0)
        { MPI_Group_incl (presentGroup, nProcNow, gMemb, &nextGroup);
          MPI_Comm_create(presentComm, nextGroup, &nextComm);
          presentGroup = nextGroup;
          presentComm = nextComm;
          if (rank >= nProcNow)
             { free (recData);
               MPI_Finalize ();
               return EXIT_SUCCESS;
             }
        }

    // Step 6: Perform scatter and gather operations
     MPI_Comm_size (presentComm, &nProc);
     MPI_Scatter (sendData, length / nProcNow, MPI_INT, recData, length / nProcNow, MPI_INT, 0, presentComm);
     printf ("Scattered data received by process %d with length = %d for a group of %d process(es).\n", rank, (length / nProcNow), nProc);
     MPI_Gather (recData, length / nProcNow, MPI_INT, sendData, length / nProcNow, MPI_INT, 0, presentComm);
     if (rank == 0)
        printf ("Gathered data received by process 0 with length = %d for a group of %d process(es).\n", length, nProc);
     
     // Reduce the number of processes for the next iteration
     nProcNow = nProcNow >> 1;
   }

  // Step 7: Clean-up and Finalization
   MPI_Finalize ();

   return EXIT_SUCCESS;
}
